{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Data Mining project: Discover and describe areas of interest<br> and events from geo-located parsed_data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #0: Import Dataset and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actual environment location may have moved due to redirects, links or junctions.\n",
      "  Requested location: \"h:\\IF4\\fouilleD\\Data-Lyon-Mining_4IF\\dataMiningEnv\\Scripts\\python.exe\"\n",
      "  Actual location:    \"\\\\ps-home.insa-lyon.fr\\users\\home\\dlarrazmar\\IF4\\fouilleD\\Data-Lyon-Mining_4IF\\dataMiningEnv\\Scripts\\python.exe\"\n",
      "'source' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "! python -m venv dataMiningEnv\n",
    "#Activate windows\n",
    "#! dataMiningEnv\\Scripts\\activate.bat\n",
    "\n",
    "#Activate mac\n",
    "! source dataMiningEnv/bin/activate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"pip'\"\n",
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.26.0\n",
      "  Using cached numpy-1.26.0-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas==2.1.1\n",
      "  Using cached pandas-2.1.1-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from pandas==2.1.1) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from pandas==2.1.1) (1.26.0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas==2.1.1) (1.17.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.1.1 pytz-2024.2 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn==1.5.1\n",
      "  Using cached scikit_learn-1.5.1-cp39-cp39-win_amd64.whl (11.0 MB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn==1.5.1) (1.26.0)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.1 scipy-1.13.1 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Program Files\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting matplotlib==3.8.0\n",
      "  Using cached matplotlib-3.8.0-cp39-cp39-win_amd64.whl (7.6 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.55.3-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached pillow-11.1.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.8.0) (24.2)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.8.0) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib==3.8.0) (2.9.0.post0)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib==3.8.0) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dlarrazmar\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib==3.8.0) (1.17.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.55.3 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.8.0 pillow-11.1.0 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "# requires ipkernel\n",
    "! pip install --upgrade pip'\n",
    "# installation of required libraries and dependencies\n",
    "# numeric calculations\n",
    "! pip install numpy==1.26.0 \n",
    "# data frames \n",
    "! pip install pandas==2.1.1 \n",
    "# machine learning algorithms \n",
    "! pip install scikit-learn==1.5.1 \n",
    "#! pip install scipy==1.12.0\n",
    "# plotting \n",
    "#! pip install plotly==5.24.1 \n",
    "! pip install matplotlib==3.8.0 \n",
    "#! pip install seaborn==0.13.2 \n",
    "#! pip install plotly-express==0.4.1 \n",
    "#! pip install chart-studio==1.1.0 \n",
    "# web app library \n",
    "#! pip install streamlit==1.37.1 \n",
    "# association rules\n",
    "#! pip install mlxtend==0.23.3\n",
    "# Language processing\n",
    "#! pip install nltk\n",
    "#! python -m nltk.downloader popular # popular functions\n",
    "# Folium\n",
    "! pip install folium==0.12.1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.cluster as cl\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "#import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données avec low_memory=False pour éviter les avertissements\n",
    "data = pd.read_csv(\"data/flickr_data2.csv\", sep=\",\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1: Discovering areas of interests using clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Write something to describe this part of the report--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clearing and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details about parsing step:\n",
    "# -> Number of lines parsed - % of original data parsed\n",
    "def parse_conclusion(parsed_data):\n",
    "    l = len(parsed_data)\n",
    "    print(f\"<Lines parsed: {l} - {round(100*l/len(data),3)}% of original data>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 1 : Nettoyage initial des colonnes\n",
    "# Supprimer les espaces supplémentaires dans les noms des colonnes\n",
    "parsed_data = data\n",
    "parsed_data.columns = parsed_data.columns.str.strip()\n",
    "\n",
    "# Convertir les colonnes temporelles en numériques\n",
    "# Liste des colonnes temporelles\n",
    "time_columns = [\n",
    "    'date_taken_minute', 'date_taken_hour', 'date_taken_day',\n",
    "    'date_taken_month', 'date_taken_year',\n",
    "    'date_upload_minute', 'date_upload_hour', 'date_upload_day',\n",
    "    'date_upload_month', 'date_upload_year'\n",
    "]\n",
    "\n",
    "# Convertir chaque colonne en int64, remplacer les erreurs par 0\n",
    "for col in time_columns:\n",
    "    parsed_data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 2 : Suppression des doublons basés sur l'identifiant unique\n",
    "# Sauvegarder les doublons pour audit futur\n",
    "duplicate_data = parsed_data[parsed_data['id'].duplicated(keep='first')].sort_values(\"id\")\n",
    "duplicate_data.to_csv(\n",
    "    \"data/parsed_lines/duplicatedId.csv\", index=False\n",
    ")\n",
    "\n",
    "# Supprimer les doublons\n",
    "parsed_data = parsed_data[~parsed_data['id'].duplicated(keep='first')]\n",
    "\n",
    "parse_conclusion(duplicate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4018163999.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    parsed_data = parsed_data[]\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Étape 3 : Gestion des colonnes inutilisées ou corrompues\n",
    "# Identifier les colonnes inutiles\n",
    "unused_columns = [\"Unnamed: 16\", \"Unnamed: 17\", \"Unnamed: 18\", \n",
    "                  \"date_upload_minute\", \"date_upload_hour\", \"date_upload_day\", \n",
    "                  \"date_upload_month\", \"date_upload_year\"]\n",
    "\n",
    "# Sauvegarder les données corrompues\n",
    "corrupted_data = parsed_data[parsed_data[unused_columns].notnull().any(axis=1)]\n",
    "corrupted_data.to_csv(\n",
    "    \"data/parsed_lines/corrupted_data.csv\"\n",
    ", index=False)\n",
    "\n",
    "\n",
    "# Supprimer les colonnes inutilisées et les lignes corrompues\n",
    "parsed_data = parsed_data[~(parsed_data[\"Unnamed: 16\"].notnull() | parsed_data[\"Unnamed: 17\"].notnull() | parsed_data[\"Unnamed: 18\"].notnull())]\n",
    "parsed_data = parsed_data.drop(columns=unused_columns)\n",
    "\n",
    "parse_conclusion(corrupted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 4 : Nettoyage des coordonnées GPS\n",
    "# Définir les limites géographiques de Lyon\n",
    "lyon_lat_min, lyon_lat_max = 45.69, 45.85\n",
    "lyon_lon_min, lyon_lon_max = 4.78, 4.92\n",
    "\n",
    "# Sauvegarder les données not Lyonnaises ou non definies\n",
    "out_lyon_data = parsed_data[\n",
    "    ~((parsed_data['lat'] >= lyon_lat_min) & \n",
    "    (parsed_data['lat'] <= lyon_lat_max) &\n",
    "    (parsed_data['long'] >= lyon_lon_min) &\n",
    "    (parsed_data['long'] <= lyon_lon_max))\n",
    "]\n",
    "out_lyon_data.to_csv(\n",
    "    \"data/parsed_lines/out_lyon.csv\"\n",
    ", index=False)\n",
    "\n",
    "# Filtrer les données pour garder uniquement les points dans Lyon\n",
    "parsed_data = parsed_data[\n",
    "    (parsed_data['lat'] >= lyon_lat_min) & \n",
    "    (parsed_data['lat'] <= lyon_lat_max) &\n",
    "    (parsed_data['long'] >= lyon_lon_min) &\n",
    "    (parsed_data['long'] <= lyon_lon_max)\n",
    "]\n",
    "\n",
    "parse_conclusion(out_lyon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 5 : Gestion des valeurs manquantes\n",
    "# Supprimer les lignes avec des valeurs manquantes\n",
    "#parsed_data = parsed_data.dropna(subset=['id', 'user', 'lat', 'long', 'tags', 'title', 'date_taken_minute', 'date_taken_hour', 'date_taken_day', 'date_taken_month', 'date_taken_year'])\n",
    "\n",
    "# Sauvegarder les lignes à données manquantes\n",
    "val_manquante_data = parsed_data[parsed_data.drop(columns=['tags','title']).isna().any(axis = 1)]\n",
    "val_manquante_data.to_csv(\n",
    "    \"data/parsed_lines/NaNs.csv\"\n",
    ", index=False)\n",
    "\n",
    "# Remplir les valeurs manquantes pour les colonnes textuelles par des chaînes vides\n",
    "parsed_data['tags'] = parsed_data['tags'].fillna('')\n",
    "parsed_data['title'] = parsed_data['title'].fillna('')\n",
    "\n",
    "parse_conclusion(val_manquante_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 6 : Filtrage des dates incohérentes\n",
    "\n",
    "# Sauvegarder les données ayant des date non correcte\n",
    "uncorrect_date_data = parsed_data[\n",
    "    ~((parsed_data['date_taken_year'] >= 2009) & (parsed_data['date_taken_year'] <= 2025) &\n",
    "    (parsed_data['date_taken_month'] >= 1) & (parsed_data['date_taken_month'] <= 12) &\n",
    "    (parsed_data['date_taken_day'] >= 1) & (parsed_data['date_taken_day'] <= 31) &\n",
    "    (parsed_data['date_taken_hour'] >= 0) & (parsed_data['date_taken_hour'] <= 23) &\n",
    "    (parsed_data['date_taken_minute'] >= 0) & (parsed_data['date_taken_minute'] <= 59))\n",
    "]\n",
    "uncorrect_date_data.to_csv(\n",
    "    \"data/parsed_lines/uncorrect_date.csv\"\n",
    ", index=False)\n",
    "\n",
    "# Garder uniquement les dates raisonnables (entre 2009 et 2025)\n",
    "parsed_data = parsed_data[\n",
    "    (parsed_data['date_taken_year'] >= 2009) & (parsed_data['date_taken_year'] <= 2025) &\n",
    "    (parsed_data['date_taken_month'] >= 1) & (parsed_data['date_taken_month'] <= 12) &\n",
    "    (parsed_data['date_taken_day'] >= 1) & (parsed_data['date_taken_day'] <= 31) &\n",
    "    (parsed_data['date_taken_hour'] >= 0) & (parsed_data['date_taken_hour'] <= 23) &\n",
    "    (parsed_data['date_taken_minute'] >= 0) & (parsed_data['date_taken_minute'] <= 59) \n",
    "]\n",
    "\n",
    "parse_conclusion(uncorrect_date_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 7 : Nettoyage/Standarisation des colonnes textuelles\n",
    "# Fonction pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    # Supprimer les caractères spéciaux et passer en minuscules\n",
    "    return re.sub(r'[^a-zA-Z0-9, ]', '', text).lower()\n",
    "\n",
    "# Appliquer le nettoyage sur les colonnes textuelles\n",
    "parsed_data['tags'] = parsed_data['tags'].apply(clean_text)\n",
    "parsed_data['title'] = parsed_data['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données nettoyées :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 388524 entries, 0 to 420239\n",
      "Data columns (total 19 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   id                  388524 non-null  int64  \n",
      " 1   user                388524 non-null  object \n",
      " 2   lat                 388524 non-null  float64\n",
      " 3   long                388524 non-null  float64\n",
      " 4   tags                388524 non-null  object \n",
      " 5   title               388524 non-null  object \n",
      " 6   date_taken_minute   388524 non-null  int64  \n",
      " 7   date_taken_hour     388524 non-null  int64  \n",
      " 8   date_taken_day      388524 non-null  int64  \n",
      " 9   date_taken_month    388524 non-null  int64  \n",
      " 10  date_taken_year     388524 non-null  int64  \n",
      " 11  date_upload_minute  388524 non-null  int64  \n",
      " 12  date_upload_hour    388524 non-null  int64  \n",
      " 13  date_upload_day     388524 non-null  int64  \n",
      " 14  date_upload_month   388524 non-null  int64  \n",
      " 15  date_upload_year    388524 non-null  int64  \n",
      " 16  Unnamed: 16         0 non-null       float64\n",
      " 17  Unnamed: 17         0 non-null       float64\n",
      " 18  Unnamed: 18         0 non-null       float64\n",
      "dtypes: float64(5), int64(11), object(3)\n",
      "memory usage: 59.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Résumé final des données nettoyées\n",
    "print(\"Données nettoyées :\")\n",
    "print(parsed_data.head(5))\n",
    "\n",
    "# Sauvegarder les données nettoyées pour les prochaines étapes\n",
    "parsed_data.to_csv(\"data/cleaned_flickr_data.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize geolocated points on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the center of Lyon\n",
    "lyon_lat, lyon_lon = 45.75, 4.85  # Approximate coordinates for Lyon, France\n",
    "\n",
    "# Create a Folium map\n",
    "map_lyon = folium.Map(location=[lyon_lat, lyon_lon], zoom_start=12)\n",
    "\n",
    "# Create a marker cluster\n",
    "marker_cluster = MarkerCluster().add_to(map_lyon)\n",
    "\n",
    "# Add points from the dataset\n",
    "for _, row in parsed_data.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['lat'], row['long']],  # Replace with 'lat' and 'long' if necessary\n",
    "        popup=f\"Tags: {row['tags']}, Title: {row['title']}\",  # Optional popup information\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Display the map\n",
    "map_lyon\n",
    "\n",
    "# Optional: Save the map as an HTML file\n",
    "map_lyon.save(\"lyon_map.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataMiningEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
